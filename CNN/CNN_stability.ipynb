{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Importing Libraries\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oarreche@ads.iu.edu/anaconda3/envs/HITL/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining Metric Equations\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Importing Libraries\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Importing Libraries')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "import numpy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import shap\n",
    "from scipy.special import softmax\n",
    "np.random.seed(0)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "#---------------------------------------------------------------------\n",
    "# Defining metric equations\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining Metric Equations')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "def print_feature_importances_shap_values(shap_values, features):\n",
    "    '''\n",
    "    Prints the feature importances based on SHAP values in an ordered way\n",
    "    shap_values -> The SHAP values calculated from a shap.Explainer object\n",
    "    features -> The name of the features, on the order presented to the explainer\n",
    "    '''\n",
    "    # Calculates the feature importance (mean absolute shap value) for each feature\n",
    "    importances = []\n",
    "    for i in range(shap_values.values.shape[1]):\n",
    "        importances.append(np.mean(np.abs(shap_values.values[:, i])))\n",
    "    # Calculates the normalized version\n",
    "    importances_norm = softmax(importances)\n",
    "    # Organize the importances and columns in a dictionary\n",
    "    feature_importances = {fea: imp for imp, fea in zip(importances, features)}\n",
    "    feature_importances_norm = {fea: imp for imp, fea in zip(importances_norm, features)}\n",
    "    # Sorts the dictionary\n",
    "    feature_importances = {k: v for k, v in sorted(feature_importances.items(), key=lambda item: item[1], reverse = True)}\n",
    "    feature_importances_norm= {k: v for k, v in sorted(feature_importances_norm.items(), key=lambda item: item[1], reverse = True)}\n",
    "    # Prints the feature importances\n",
    "    for k, v in feature_importances.items():\n",
    "        print(f\"{k} -> {v:.4f} (softmax = {feature_importances_norm[k]:.4f})\")\n",
    "\n",
    "\n",
    "def ACC(TP,TN,FP,FN):\n",
    "    Acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    return Acc\n",
    "def ACC_2 (TP, FN):\n",
    "    ac = (TP/(TP+FN))\n",
    "    return ac\n",
    "def PRECISION(TP,FP):\n",
    "    Precision = TP/(TP+FP)\n",
    "    return Precision\n",
    "def RECALL(TP,FN):\n",
    "    Recall = TP/(TP+FN)\n",
    "    return Recall\n",
    "def F1(Recall, Precision):\n",
    "    F1 = 2 * Recall * Precision / (Recall + Precision)\n",
    "    return F1\n",
    "def BACC(TP,TN,FP,FN):\n",
    "    BACC =(TP/(TP+FN)+ TN/(TN+FP))*0.5\n",
    "    return BACC\n",
    "def MCC(TP,TN,FP,FN):\n",
    "    MCC = (TN*TP-FN*FP)/(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))**.5)\n",
    "    return MCC\n",
    "def AUC_ROC(y_test_bin,y_score):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    auc_avg = 0\n",
    "    counting = 0\n",
    "    for i in range(n_classes):\n",
    "      fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "      auc_avg += auc(fpr[i], tpr[i])\n",
    "      counting = i+1\n",
    "    return auc_avg/counting\n",
    "\n",
    "def oversample(X_train, y_train):\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    # Convert to numpy and oversample\n",
    "    x_np = X_train.to_numpy()\n",
    "    y_np = y_train.to_numpy()\n",
    "    x_np, y_np = oversample.fit_resample(x_np, y_np)\n",
    "\n",
    "    # Convert back to pandas\n",
    "    x_over = pd.DataFrame(x_np, columns=X_train.columns)\n",
    "    y_over = pd.Series(y_np)\n",
    "    return x_over, y_over\n",
    "\n",
    "#---------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Generating Stability Comparison\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "RF SHAP CICIDS GLOBAL\n",
      "---------------------------------------------------------------------------------\n",
      "0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Generating Stability Comparison')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF #### RF ####\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CNN LIME CICIDS GLOBAL')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "Trial_1 =[\n",
    "' Bwd Packet Length Mean',\n",
    "' Bwd IAT Std',\n",
    "' Average Packet Size',\n",
    "' Bwd Packets/s',\n",
    "' Bwd PSH Flags',\n",
    "' Bwd IAT Min',\n",
    "' Down/Up Ratio',\n",
    "' Bwd Packet Length Min',\n",
    "' Bwd Packet Length Std',\n",
    "' Active Max',\n",
    "' Active Std',\n",
    "# ' Avg Bwd Segment Size',\n",
    "# ' Bwd Avg Bytes/Bulk',\n",
    "# ' Flow IAT Min',\n",
    "# ' Bwd Avg Packets/Bulk',\n",
    "# ' Bwd URG Flags',\n",
    "# ' ACK Flag Count',\n",
    "# ' Bwd IAT Max',\n",
    "# ' Active Min',\n",
    "# ' Bwd Header Length',\n",
    "# ' Bwd IAT Mean',\n",
    "# ' Destination Port',\n",
    "# ' ECE Flag Count',\n",
    "# ' CWE Flag Count',\n",
    "# ' Avg Fwd Segment Size',\n",
    "# ' Flow IAT Max',\n",
    "# ' Flow Packets/s',\n",
    "# ' Flow IAT Std',\n",
    "# ' Flow IAT Mean',\n",
    "# ' Fwd Avg Bulk Rate',\n",
    "# ' Flow Duration',\n",
    "# ' Fwd Avg Packets/Bulk',\n",
    "]\n",
    "\n",
    "Trial_2 =[\n",
    "\n",
    "' Bwd IAT Std',\n",
    "' Down/Up Ratio',\n",
    "' Average Packet Size',\n",
    "' Bwd PSH Flags',\n",
    "' Bwd Packet Length Min',\n",
    "' Bwd URG Flags',\n",
    "' Bwd Packet Length Mean',\n",
    "' Bwd Avg Bytes/Bulk',\n",
    "' Bwd IAT Mean',\n",
    "' Avg Bwd Segment Size',\n",
    "' Bwd IAT Max',\n",
    "# ' Flow IAT Min',\n",
    "# ' CWE Flag Count',\n",
    "# ' Bwd Avg Packets/Bulk',\n",
    "# ' Active Std',\n",
    "# ' Bwd Packet Length Std',\n",
    "# ' Fwd Avg Bulk Rate',\n",
    "# ' Bwd IAT Min',\n",
    "# ' Active Max',\n",
    "# ' Flow Packets/s',\n",
    "# ' Flow IAT Std',\n",
    "# ' ECE Flag Count',\n",
    "# ' Bwd Header Length',\n",
    "# ' Bwd Packets/s',\n",
    "# ' ACK Flag Count',\n",
    "# ' Destination Port',\n",
    "# ' Active Min',\n",
    "# ' Flow IAT Mean',\n",
    "# ' Avg Fwd Segment Size',\n",
    "# ' Flow IAT Max',\n",
    "# ' Flow Duration',\n",
    "# ' Fwd Avg Packets/Bulk',\n",
    "\n",
    "]\n",
    "\n",
    "Trial_3 =[\n",
    "\n",
    "' Bwd IAT Std',\n",
    "' Bwd Packet Length Mean',\n",
    "' Average Packet Size',\n",
    "' Bwd Packet Length Min',\n",
    "' Down/Up Ratio',\n",
    "' Bwd PSH Flags',\n",
    "' Bwd IAT Min',\n",
    "' Active Std',\n",
    "' ACK Flag Count',\n",
    "' Avg Bwd Segment Size',\n",
    "' Bwd Avg Bytes/Bulk',\n",
    "# ' Bwd Packet Length Std',\n",
    "# ' Bwd Packets/s',\n",
    "# ' CWE Flag Count',\n",
    "# ' Destination Port',\n",
    "# ' Active Min',\n",
    "# ' Bwd Header Length',\n",
    "# ' Flow IAT Max',\n",
    "# ' Bwd URG Flags',\n",
    "# ' Active Max',\n",
    "# ' Bwd Avg Packets/Bulk',\n",
    "# ' Bwd IAT Mean',\n",
    "# ' ECE Flag Count',\n",
    "# ' Flow IAT Min',\n",
    "# ' Bwd IAT Max',\n",
    "# ' Flow Packets/s',\n",
    "# ' Avg Fwd Segment Size',\n",
    "# ' Flow IAT Std',\n",
    "# ' Flow IAT Mean',\n",
    "# ' Flow Duration',\n",
    "# ' Fwd Avg Bulk Rate',\n",
    "# ' Fwd Avg Packets/Bulk',\n",
    "]\n",
    "\n",
    "Intersection123 = set(Trial_1) & set(Trial_2) & set(Trial_3)\n",
    "\n",
    "Total = len(Trial_1)\n",
    "\n",
    "RF_stab = len(Intersection123)/Total\n",
    "print(RF_stab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HITL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
