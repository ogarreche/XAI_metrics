{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from adversarial_models import * \n",
    "from utils import *\n",
    "from get_data import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction = 0.01\n",
    "# frac_normal = 0.3\n",
    "\n",
    "# req_cols = [' Init_Win_bytes_backward',\n",
    "#                         ' Destination Port',' Fwd Packet Length Std',\n",
    "#                         ' Flow IAT Max','Total Length of Fwd Packets',' Flow Duration', ' Label']\n",
    "\n",
    "# cols = req_cols\n",
    "# df0 = pd.read_csv ('cicids_db/Wednesday-workingHours.pcap_ISCX.csv', usecols=req_cols)\n",
    "# df1 = pd.read_csv ('cicids_db/Tuesday-WorkingHours.pcap_ISCX.csv', usecols=req_cols)\n",
    "# df2 = pd.read_csv ('cicids_db/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv', usecols=req_cols)\n",
    "# df3 = pd.read_csv ('cicids_db/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv', usecols=req_cols)\n",
    "# df4 = pd.read_csv ('cicids_db/Monday-WorkingHours.pcap_ISCX.csv', usecols=req_cols)\n",
    "# df5 = pd.read_csv ('cicids_db/Friday-WorkingHours-Morning.pcap_ISCX.csv', usecols=req_cols)\n",
    "# df6 = pd.read_csv ('cicids_db/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', usecols=req_cols)\n",
    "# df7 = pd.read_csv ('cicids_db/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', usecols=req_cols)\n",
    "\n",
    "# frames = [df0, df1, df2, df3, df4, df5,df6, df7]\n",
    "\n",
    "# df = pd.concat(frames,ignore_index=True)\n",
    "# df = df.sample(frac = fraction)\n",
    "\n",
    "# y = df.pop(' Label')\n",
    "# df = df.assign(Label = y)\n",
    "\n",
    "# print('reached')\n",
    "\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# print('Reducing Normal rows')\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# print('')\n",
    "\n",
    "\n",
    "# #filters\n",
    "\n",
    "# filtered_normal = df[df['Label'] == 'BENIGN']\n",
    "\n",
    "# #reduce\n",
    "\n",
    "# reduced_normal = filtered_normal.sample(frac=frac_normal)\n",
    "\n",
    "# #join\n",
    "\n",
    "# df = pd.concat([df[df['Label'] != 'BENIGN'], reduced_normal])\n",
    "\n",
    "# ''' ---------------------------------------------------------------'''\n",
    "# df_max_scaled = df.copy()\n",
    "\n",
    "\n",
    "# y = df_max_scaled['Label'].replace({'DoS GoldenEye': 'Dos/Ddos', 'DoS Hulk': 'Dos/Ddos', 'DoS Slowhttptest': 'Dos/Ddos', 'DoS slowloris': 'Dos/Ddos', 'Heartbleed': 'Dos/Ddos', 'DDoS': 'Dos/Ddos','FTP-Patator': 'Brute Force', 'SSH-Patator': 'Brute Force','Web Attack - Brute Force': 'Web Attack', 'Web Attack - Sql Injection': 'Web Attack', 'Web Attack - XSS': 'Web Attack'})\n",
    "\n",
    "# df_max_scaled.pop('Label')\n",
    "\n",
    "# print('reached2')\n",
    "\n",
    "# df = df.assign(Label = y)\n",
    "\n",
    "\n",
    "# X = df\n",
    "# y = X['Label']\n",
    "\n",
    "# y,label = pd.factorize(y)\n",
    "# y = pd.DataFrame(y)\n",
    "# label = list(label)\n",
    "\n",
    "# df ['Label'] = y \n",
    "# X = X.drop([\"Label\"], axis=1)\n",
    "# print(label.index('BENIGN'))\n",
    "# X[' Flow Duration'] = [label.index('BENIGN') if v > 10000 else label.index('Dos/Ddos')  for v in X[' Flow Duration'].values]\n",
    "\n",
    "# # y = np.array([POSITIVE_OUTCOME if p == 0 else NEGATIVE_OUTCOME for p in y.values])\n",
    "# # y = np.array([POSITIVE_OUTCOME if p == 1 else NEGATIVE_OUTCOME for p in y.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the Training set: (125973, 43)\n",
      "Save\n",
      "        duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
      "0              0              1       20     9        491          0     0   \n",
      "1              0              2       44     9        146          0     0   \n",
      "2              0              1       49     5          0          0     0   \n",
      "3              0              1       24     9        232       8153     0   \n",
      "4              0              1       24     9        199        420     0   \n",
      "...          ...            ...      ...   ...        ...        ...   ...   \n",
      "125968         0              1       49     5          0          0     0   \n",
      "125969         8              2       49     9        105        145     0   \n",
      "125970         0              1       54     9       2231        384     0   \n",
      "125971         0              1       30     5          0          0     0   \n",
      "125972         0              1       20     9        151          0     0   \n",
      "\n",
      "        wrong_fragment  urgent  hot  num_failed_logins  logged_in  \\\n",
      "0                    0       0    0                  0          0   \n",
      "1                    0       0    0                  0          0   \n",
      "2                    0       0    0                  0          0   \n",
      "3                    0       0    0                  0          1   \n",
      "4                    0       0    0                  0          1   \n",
      "...                ...     ...  ...                ...        ...   \n",
      "125968               0       0    0                  0          0   \n",
      "125969               0       0    0                  0          0   \n",
      "125970               0       0    0                  0          1   \n",
      "125971               0       0    0                  0          0   \n",
      "125972               0       0    0                  0          1   \n",
      "\n",
      "        num_compromised  root_shell  su_attempted  num_root  \\\n",
      "0                     0           0             0         0   \n",
      "1                     0           0             0         0   \n",
      "2                     0           0             0         0   \n",
      "3                     0           0             0         0   \n",
      "4                     0           0             0         0   \n",
      "...                 ...         ...           ...       ...   \n",
      "125968                0           0             0         0   \n",
      "125969                0           0             0         0   \n",
      "125970                0           0             0         0   \n",
      "125971                0           0             0         0   \n",
      "125972                0           0             0         0   \n",
      "\n",
      "        num_file_creations  num_shells  num_access_files  num_outbound_cmds  \\\n",
      "0                        0           0                 0                  0   \n",
      "1                        0           0                 0                  0   \n",
      "2                        0           0                 0                  0   \n",
      "3                        0           0                 0                  0   \n",
      "4                        0           0                 0                  0   \n",
      "...                    ...         ...               ...                ...   \n",
      "125968                   0           0                 0                  0   \n",
      "125969                   0           0                 0                  0   \n",
      "125970                   0           0                 0                  0   \n",
      "125971                   0           0                 0                  0   \n",
      "125972                   0           0                 0                  0   \n",
      "\n",
      "        is_host_login  is_guest_login  count  srv_count  serror_rate  \\\n",
      "0                   0               0      2          2          0.0   \n",
      "1                   0               0     13          1          0.0   \n",
      "2                   0               0    123          6          1.0   \n",
      "3                   0               0      5          5          0.2   \n",
      "4                   0               0     30         32          0.0   \n",
      "...               ...             ...    ...        ...          ...   \n",
      "125968              0               0    184         25          1.0   \n",
      "125969              0               0      2          2          0.0   \n",
      "125970              0               0      1          1          0.0   \n",
      "125971              0               0    144          8          1.0   \n",
      "125972              0               0      1          1          0.0   \n",
      "\n",
      "        srv_serror_rate  rerror_rate  srv_rerror_rate  same_srv_rate  \\\n",
      "0                   0.0          0.0              0.0           1.00   \n",
      "1                   0.0          0.0              0.0           0.08   \n",
      "2                   1.0          0.0              0.0           0.05   \n",
      "3                   0.2          0.0              0.0           1.00   \n",
      "4                   0.0          0.0              0.0           1.00   \n",
      "...                 ...          ...              ...            ...   \n",
      "125968              1.0          0.0              0.0           0.14   \n",
      "125969              0.0          0.0              0.0           1.00   \n",
      "125970              0.0          0.0              0.0           1.00   \n",
      "125971              1.0          0.0              0.0           0.06   \n",
      "125972              0.0          0.0              0.0           1.00   \n",
      "\n",
      "        diff_srv_rate  srv_diff_host_rate  dst_host_count  dst_host_srv_count  \\\n",
      "0                0.00                0.00             150                  25   \n",
      "1                0.15                0.00             255                   1   \n",
      "2                0.07                0.00             255                  26   \n",
      "3                0.00                0.00              30                 255   \n",
      "4                0.00                0.09             255                 255   \n",
      "...               ...                 ...             ...                 ...   \n",
      "125968           0.06                0.00             255                  25   \n",
      "125969           0.00                0.00             255                 244   \n",
      "125970           0.00                0.00             255                  30   \n",
      "125971           0.05                0.00             255                   8   \n",
      "125972           0.00                0.00             255                  77   \n",
      "\n",
      "        dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                            1                    0.03   \n",
      "1                            1                    0.60   \n",
      "2                            1                    0.05   \n",
      "3                            0                    0.00   \n",
      "4                            0                    0.00   \n",
      "...                        ...                     ...   \n",
      "125968                       1                    0.06   \n",
      "125969                       0                    0.01   \n",
      "125970                       1                    0.06   \n",
      "125971                       1                    0.05   \n",
      "125972                       1                    0.03   \n",
      "\n",
      "        dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                              0.17                         0.00   \n",
      "1                              0.88                         0.00   \n",
      "2                              0.00                         0.00   \n",
      "3                              0.03                         0.04   \n",
      "4                              0.00                         0.00   \n",
      "...                             ...                          ...   \n",
      "125968                         0.00                         0.00   \n",
      "125969                         0.01                         0.00   \n",
      "125970                         0.00                         0.00   \n",
      "125971                         0.00                         0.00   \n",
      "125972                         0.30                         0.00   \n",
      "\n",
      "        dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                       0.00                      0.00                  0.05   \n",
      "1                       0.00                      0.00                  0.00   \n",
      "2                       1.00                      1.00                  0.00   \n",
      "3                       0.03                      0.01                  0.00   \n",
      "4                       0.00                      0.00                  0.00   \n",
      "...                      ...                       ...                   ...   \n",
      "125968                  1.00                      1.00                  0.00   \n",
      "125969                  0.00                      0.00                  0.00   \n",
      "125970                  0.72                      0.00                  0.01   \n",
      "125971                  1.00                      1.00                  0.00   \n",
      "125972                  0.00                      0.00                  0.00   \n",
      "\n",
      "        dst_host_srv_rerror_rate  \n",
      "0                           0.00  \n",
      "1                           0.00  \n",
      "2                           0.00  \n",
      "3                           0.01  \n",
      "4                           0.00  \n",
      "...                          ...  \n",
      "125968                      0.00  \n",
      "125969                      0.00  \n",
      "125970                      0.00  \n",
      "125971                      0.00  \n",
      "125972                      0.00  \n",
      "\n",
      "[125973 rows x 41 columns]\n",
      "125973\n"
     ]
    }
   ],
   "source": [
    "# Get the data set and do some preprocessing\n",
    "# cols = [' Init_Win_bytes_backward',\n",
    "#         ' Destination Port',' Fwd Packet Length Std',\n",
    "#         ' Flow IAT Max','Total Length of Fwd Packets']\n",
    "\n",
    "# X = df[cols]  # Select the features from df\n",
    "# y = df[' Label']  # Replace 'target_column' with the actual name of your target column\n",
    "\n",
    "params = Params(\"model_configurations/experiment_params.json\")\n",
    "np.random.seed(params.seed)\n",
    "X, y, cols = get_and_preprocess_nsl(params)\n",
    "\n",
    "\n",
    "# Add a random column -- this is what we'll have LIME/SHAP explain.\n",
    "X['unrelated_column'] = np.random.choice([0,1],size=X.shape[0])\n",
    "features = [c for c in X]\n",
    "\n",
    "categorical_feature_name = [\"service\",\n",
    "                            'unrelated_column']\n",
    "\n",
    "\n",
    "categorical_feature_indcs = [features.index(c) for c in categorical_feature_name]\n",
    "\n",
    "race_indc = features.index('dst_host_same_srv_rate')\n",
    "unrelated_indcs = features.index('unrelated_column')\n",
    "X = X.values\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class racist_model_f:\n",
    "    # Decision rule: classify negatively if race is black\n",
    "    def predict(self,X):\n",
    "        return np.array([params.negative_outcome if x[race_indc] > 0 else params.positive_outcome for x in X])\n",
    "\n",
    "    def predict_proba(self, X): \n",
    "        return one_hot_encode(self.predict(X))\n",
    "\n",
    "    def score(self, X,y):\n",
    "        return np.sum(self.predict(X)==y) / len(X)\n",
    "    \n",
    "class innocuous_model_psi:\n",
    "    # Decision rule: classify according to randomly drawn column 'unrelated column'\n",
    "    def predict(self,X):\n",
    "        return np.array([params.negative_outcome if x[unrelated_indcs] > 0 else params.positive_outcome for x in X])\n",
    "\n",
    "    def predict_proba(self, X): \n",
    "        return one_hot_encode(self.predict(X))\n",
    "\n",
    "    def score(self, X,y):\n",
    "        return np.sum(self.predict(X)==y) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Now you can use train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data and normalize\n",
    "\n",
    "xtest_not_normalized = deepcopy(xtest)\n",
    "ss = StandardScaler().fit(xtrain)\n",
    "xtrain = ss.transform(xtrain)\n",
    "xtest = ss.transform(xtest)\n",
    "\n",
    "# Train the adversarial model for LIME with f and psi \n",
    "adv_lime = Adversarial_Lime_Model(racist_model_f(), innocuous_model_psi()).\\\n",
    "            train(xtrain[:10000], ytrain[:10000], feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "real value [1]\n",
      "------------------------------------------------------------------\n",
      "------------------------------------------------------------------\n",
      "predicted value 1\n",
      "------------------------------------------------------------------\n",
      "Explanation on biased f:\n",
      " [('dst_host_same_srv_rate', -0.40413157420925533), ('dst_host_rerror_rate', -0.011851806551820186), ('service', 0.011400811297481685)] \n",
      "\n",
      "\n",
      "Explanation on adversarial model:\n",
      " [('unrelated_column', -0.40631179166714804), ('srv_rerror_rate', 0.00924501879853029), ('dst_host_rerror_rate', -0.006885625643964393)] \n",
      "\n",
      "Prediction fidelity: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Let's just look at a the first example in the test set\n",
    "ex_indc = np.random.choice(xtest.shape[0])\n",
    "# print(xtest.shape[0])\n",
    "indices = np.where(ytest == 1)\n",
    "index_positions = indices[0]\n",
    "# print(index_positions)\n",
    "ex_indc = index_positions[0]\n",
    "# To get a baseline, we'll look at LIME applied to the biased model f\n",
    "normal_explainer = lime.lime_tabular.LimeTabularExplainer(xtrain,feature_names=adv_lime.get_column_names(),\n",
    "                                                          discretize_continuous=False)\n",
    "\n",
    "normal_exp = normal_explainer.explain_instance(xtest[ex_indc], racist_model_f().predict_proba).as_list()\n",
    "\n",
    "print('------------------------------------------------------------------')\n",
    "print('real value', ytest[ex_indc:ex_indc+1])\n",
    "# print(ytest)\n",
    "print('------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------')\n",
    "# print(xtest[ex_indc])\n",
    "# print(xtest)\n",
    "print('predicted value', racist_model_f().predict(xtest)[ex_indc])\n",
    "\n",
    "print('------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "# pred = racist_model_f().predict_proba(xtest[ex_indc])                             \n",
    "# print('prediction', pred)\n",
    "print (\"Explanation on biased f:\\n\",normal_exp[:3],\"\\n\\n\")\n",
    "\n",
    "# Now, lets look at the explanations on the adversarial model \n",
    "adv_explainer = lime.lime_tabular.LimeTabularExplainer(xtrain,feature_names=adv_lime.get_column_names(), \n",
    "                                                       discretize_continuous=False)\n",
    "\n",
    "adv_exp = adv_explainer.explain_instance(xtest[ex_indc], adv_lime.predict_proba).as_list()\n",
    "\n",
    "print (\"Explanation on adversarial model:\\n\",adv_exp[:3],\"\\n\")\n",
    "\n",
    "print(\"Prediction fidelity: {0:3.2}\".format(adv_lime.fidelity(xtest[ex_indc:ex_indc+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 buckets, 3 graphs\n",
    "dict_biased_1 = {\n",
    "    'Biased': 0,\n",
    "    'Unrelated_1':0,\n",
    "    'Unrelated_2':0,\n",
    "    'Others':0\n",
    "}\n",
    "dict_biased_2 = {\n",
    "    'Biased': 0,\n",
    "    'Unrelated_1':0,\n",
    "    'Unrelated_2':0,\n",
    "    'Others':0\n",
    "}\n",
    "dict_biased_3 = {\n",
    "    'Biased': 0,\n",
    "    'Unrelated_1':0,\n",
    "    'Unrelated_2':0,\n",
    "    'Others':0\n",
    "}\n",
    "attack_1 = {\n",
    "    'Biased': 0,\n",
    "    'Unrelated_1':0,\n",
    "    'Unrelated_2':0,\n",
    "    'Others':0\n",
    "}\n",
    "\n",
    "attack_2 = {\n",
    "    'Biased': 0,\n",
    "    'Unrelated_1':0,\n",
    "    'Unrelated_2':0,\n",
    "    'Others':0\n",
    "}\n",
    "\n",
    "attack_3 = {\n",
    "    'Biased': 0,\n",
    "    'Unrelated_1':0,\n",
    "    'Unrelated_2':0,\n",
    "    'Others':0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME Results\n",
    "What we have is the top three features of the explanation on the biased model followed by the explanation on that same instance using the adversarial model. We see that the top feature is now the randomly drawn column, indicating that the attack was able to fool LIME. We also see that the adversarial model predicts this instance _the same_ as the biased model (fidelity=1)â€”although the model predicts the same results, its reasoning has changed.\n",
    "\n",
    "Next, let's look at the SHAP adversarial model.  We'll go through a similar process as above except using SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features analyzed\n",
    "biased_feature = 'dst_host_same_srv_rate'\n",
    "Unrelated_1_feature = 'unrelated_column'\n",
    "# Unrelated_2_feature = 'unrelated_column'\n",
    "samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['count', 'diff_srv_rate', 'dst_bytes', 'dst_host_count', 'dst_host_diff_srv_rate', 'dst_host_rerror_rate', 'dst_host_same_src_port_rate', 'dst_host_same_srv_rate', 'dst_host_serror_rate', 'dst_host_srv_count', 'dst_host_srv_diff_host_rate', 'dst_host_srv_rerror_rate', 'dst_host_srv_serror_rate', 'duration', 'flag', 'hot', 'is_guest_login', 'is_host_login', 'land', 'logged_in', 'num_access_files', 'num_compromised', 'num_failed_logins', 'num_file_creations', 'num_outbound_cmds', 'num_root', 'num_shells', 'protocol_type', 'rerror_rate', 'root_shell', 'same_srv_rate', 'serror_rate', 'service', 'src_bytes', 'srv_count', 'srv_diff_host_rate', 'srv_rerror_rate', 'srv_serror_rate', 'su_attempted', 'unrelated_column', 'urgent', 'wrong_fragment']\n",
      "progress 0.1 %\n",
      "progress 0.2 %\n",
      "progress 0.3 %\n",
      "progress 0.4 %\n",
      "progress 0.5 %\n",
      "progress 0.6 %\n",
      "progress 0.7000000000000001 %\n",
      "progress 0.8 %\n",
      "progress 0.8999999999999999 %\n",
      "progress 1.0 %\n",
      "progress 1.0999999999999999 %\n",
      "progress 1.2 %\n",
      "progress 1.3 %\n",
      "progress 1.4000000000000001 %\n",
      "progress 1.5 %\n",
      "progress 1.6 %\n",
      "progress 1.7000000000000002 %\n",
      "progress 1.7999999999999998 %\n",
      "progress 1.9 %\n",
      "progress 2.0 %\n",
      "progress 2.1 %\n",
      "progress 2.1999999999999997 %\n",
      "progress 2.3 %\n",
      "progress 2.4 %\n",
      "progress 2.5 %\n",
      "progress 2.6 %\n",
      "progress 2.7 %\n",
      "progress 2.8000000000000003 %\n",
      "progress 2.9000000000000004 %\n",
      "progress 3.0 %\n",
      "progress 3.1 %\n",
      "progress 3.2 %\n",
      "progress 3.3000000000000003 %\n",
      "progress 3.4000000000000004 %\n",
      "progress 3.5000000000000004 %\n",
      "progress 3.5999999999999996 %\n",
      "progress 3.6999999999999997 %\n",
      "progress 3.8 %\n",
      "progress 3.9 %\n",
      "progress 4.0 %\n",
      "progress 4.1000000000000005 %\n",
      "progress 4.2 %\n",
      "progress 4.3 %\n",
      "progress 4.3999999999999995 %\n",
      "progress 4.5 %\n",
      "progress 4.6 %\n",
      "progress 4.7 %\n",
      "progress 4.8 %\n",
      "progress 4.9 %\n",
      "progress 5.0 %\n",
      "progress 5.1 %\n",
      "progress 5.2 %\n",
      "progress 5.3 %\n",
      "progress 5.4 %\n",
      "progress 5.5 %\n",
      "progress 5.6000000000000005 %\n",
      "progress 5.7 %\n",
      "progress 5.800000000000001 %\n",
      "progress 5.8999999999999995 %\n",
      "progress 6.0 %\n",
      "progress 6.1 %\n",
      "progress 6.2 %\n",
      "progress 6.3 %\n",
      "progress 6.4 %\n",
      "progress 6.5 %\n",
      "progress 6.6000000000000005 %\n",
      "progress 6.7 %\n",
      "progress 6.800000000000001 %\n",
      "progress 6.9 %\n",
      "progress 7.000000000000001 %\n",
      "progress 7.1 %\n",
      "progress 7.199999999999999 %\n",
      "progress 7.3 %\n",
      "progress 7.3999999999999995 %\n",
      "progress 7.5 %\n",
      "progress 7.6 %\n",
      "progress 7.7 %\n",
      "progress 7.8 %\n",
      "progress 7.9 %\n",
      "progress 8.0 %\n",
      "progress 8.1 %\n",
      "progress 8.200000000000001 %\n",
      "progress 8.3 %\n",
      "progress 8.4 %\n",
      "progress 8.5 %\n",
      "progress 8.6 %\n",
      "progress 8.7 %\n",
      "progress 8.799999999999999 %\n",
      "progress 8.9 %\n",
      "progress 9.0 %\n",
      "progress 9.1 %\n",
      "progress 9.2 %\n",
      "progress 9.3 %\n",
      "progress 9.4 %\n",
      "progress 9.5 %\n",
      "progress 9.6 %\n",
      "progress 9.700000000000001 %\n",
      "progress 9.8 %\n",
      "progress 9.9 %\n",
      "progress 10.0 %\n",
      "progress 10.100000000000001 %\n",
      "progress 10.2 %\n",
      "progress 10.299999999999999 %\n",
      "progress 10.4 %\n",
      "progress 10.5 %\n",
      "progress 10.6 %\n",
      "progress 10.7 %\n",
      "progress 10.8 %\n",
      "progress 10.9 %\n",
      "progress 11.0 %\n",
      "progress 11.1 %\n",
      "progress 11.200000000000001 %\n",
      "progress 11.3 %\n",
      "progress 11.4 %\n",
      "progress 11.5 %\n",
      "progress 11.600000000000001 %\n",
      "progress 11.700000000000001 %\n",
      "progress 11.799999999999999 %\n",
      "progress 11.899999999999999 %\n",
      "progress 12.0 %\n",
      "progress 12.1 %\n",
      "progress 12.2 %\n",
      "progress 12.3 %\n",
      "progress 12.4 %\n",
      "progress 12.5 %\n",
      "progress 12.6 %\n",
      "progress 12.7 %\n",
      "progress 12.8 %\n",
      "progress 12.9 %\n",
      "progress 13.0 %\n",
      "progress 13.100000000000001 %\n",
      "progress 13.200000000000001 %\n",
      "progress 13.3 %\n",
      "progress 13.4 %\n",
      "progress 13.5 %\n",
      "progress 13.600000000000001 %\n",
      "progress 13.700000000000001 %\n",
      "progress 13.8 %\n",
      "progress 13.900000000000002 %\n",
      "progress 14.000000000000002 %\n",
      "progress 14.099999999999998 %\n",
      "progress 14.2 %\n",
      "progress 14.299999999999999 %\n",
      "progress 14.399999999999999 %\n",
      "progress 14.499999999999998 %\n",
      "progress 14.6 %\n",
      "progress 14.7 %\n",
      "progress 14.799999999999999 %\n",
      "progress 14.899999999999999 %\n",
      "progress 15.0 %\n",
      "progress 15.1 %\n",
      "progress 15.2 %\n",
      "progress 15.299999999999999 %\n",
      "progress 15.4 %\n",
      "progress 15.5 %\n",
      "progress 15.6 %\n",
      "progress 15.7 %\n",
      "progress 15.8 %\n",
      "progress 15.9 %\n",
      "progress 16.0 %\n",
      "progress 16.1 %\n",
      "progress 16.2 %\n",
      "progress 16.3 %\n",
      "progress 16.400000000000002 %\n",
      "progress 16.5 %\n",
      "progress 16.6 %\n",
      "progress 16.7 %\n",
      "progress 16.8 %\n",
      "progress 16.900000000000002 %\n",
      "progress 17.0 %\n",
      "progress 17.1 %\n",
      "progress 17.2 %\n",
      "progress 17.299999999999997 %\n",
      "progress 17.4 %\n",
      "progress 17.5 %\n",
      "progress 17.599999999999998 %\n",
      "progress 17.7 %\n",
      "progress 17.8 %\n",
      "progress 17.9 %\n",
      "progress 18.0 %\n",
      "progress 18.099999999999998 %\n",
      "progress 18.2 %\n",
      "progress 18.3 %\n",
      "progress 18.4 %\n",
      "progress 18.5 %\n",
      "progress 18.6 %\n",
      "progress 18.7 %\n",
      "progress 18.8 %\n",
      "progress 18.9 %\n",
      "progress 19.0 %\n",
      "progress 19.1 %\n",
      "progress 19.2 %\n",
      "progress 19.3 %\n",
      "progress 19.400000000000002 %\n",
      "progress 19.5 %\n",
      "progress 19.6 %\n",
      "progress 19.7 %\n",
      "progress 19.8 %\n",
      "progress 19.900000000000002 %\n",
      "progress 20.0 %\n",
      "progress 20.1 %\n",
      "progress 20.200000000000003 %\n",
      "progress 20.3 %\n",
      "progress 20.4 %\n",
      "progress 20.5 %\n",
      "progress 20.599999999999998 %\n",
      "progress 20.7 %\n",
      "progress 20.8 %\n",
      "progress 20.9 %\n",
      "progress 21.0 %\n",
      "progress 21.099999999999998 %\n",
      "progress 21.2 %\n",
      "progress 21.3 %\n",
      "progress 21.4 %\n",
      "progress 21.5 %\n",
      "progress 21.6 %\n",
      "progress 21.7 %\n",
      "progress 21.8 %\n",
      "progress 21.9 %\n",
      "progress 22.0 %\n",
      "progress 22.1 %\n",
      "progress 22.2 %\n",
      "progress 22.3 %\n",
      "progress 22.400000000000002 %\n",
      "progress 22.5 %\n",
      "progress 22.6 %\n",
      "progress 22.7 %\n",
      "progress 22.8 %\n",
      "progress 22.900000000000002 %\n",
      "progress 23.0 %\n",
      "progress 23.1 %\n",
      "progress 23.200000000000003 %\n",
      "progress 23.3 %\n",
      "progress 23.400000000000002 %\n",
      "progress 23.5 %\n",
      "progress 23.599999999999998 %\n",
      "progress 23.7 %\n",
      "progress 23.799999999999997 %\n",
      "progress 23.9 %\n",
      "progress 24.0 %\n",
      "progress 24.099999999999998 %\n",
      "progress 24.2 %\n",
      "progress 24.3 %\n",
      "progress 24.4 %\n",
      "progress 24.5 %\n",
      "progress 24.6 %\n",
      "progress 24.7 %\n",
      "progress 24.8 %\n",
      "progress 24.9 %\n",
      "progress 25.0 %\n",
      "progress 25.1 %\n",
      "progress 25.2 %\n",
      "progress 25.3 %\n",
      "progress 25.4 %\n",
      "progress 25.5 %\n",
      "progress 25.6 %\n",
      "progress 25.7 %\n",
      "progress 25.8 %\n",
      "progress 25.900000000000002 %\n",
      "progress 26.0 %\n",
      "progress 26.1 %\n",
      "progress 26.200000000000003 %\n",
      "progress 26.3 %\n",
      "progress 26.400000000000002 %\n",
      "progress 26.5 %\n",
      "progress 26.6 %\n",
      "progress 26.700000000000003 %\n",
      "progress 26.8 %\n",
      "progress 26.900000000000002 %\n",
      "progress 27.0 %\n",
      "progress 27.1 %\n",
      "progress 27.200000000000003 %\n",
      "progress 27.3 %\n",
      "progress 27.400000000000002 %\n",
      "progress 27.500000000000004 %\n",
      "progress 27.6 %\n",
      "progress 27.700000000000003 %\n",
      "progress 27.800000000000004 %\n",
      "progress 27.900000000000002 %\n",
      "progress 28.000000000000004 %\n",
      "progress 28.1 %\n",
      "progress 28.199999999999996 %\n",
      "progress 28.299999999999997 %\n",
      "progress 28.4 %\n",
      "progress 28.499999999999996 %\n",
      "progress 28.599999999999998 %\n",
      "progress 28.7 %\n",
      "progress 28.799999999999997 %\n",
      "progress 28.9 %\n",
      "progress 28.999999999999996 %\n",
      "progress 29.099999999999998 %\n",
      "progress 29.2 %\n",
      "progress 29.299999999999997 %\n",
      "progress 29.4 %\n",
      "progress 29.5 %\n",
      "progress 29.599999999999998 %\n",
      "progress 29.7 %\n",
      "progress 29.799999999999997 %\n",
      "progress 29.9 %\n",
      "progress 30.0 %\n",
      "progress 30.099999999999998 %\n",
      "progress 30.2 %\n",
      "progress 30.3 %\n",
      "progress 30.4 %\n",
      "progress 30.5 %\n",
      "progress 30.599999999999998 %\n",
      "progress 30.7 %\n",
      "progress 30.8 %\n",
      "progress 30.9 %\n",
      "progress 31.0 %\n",
      "progress 31.1 %\n",
      "progress 31.2 %\n",
      "progress 31.3 %\n",
      "progress 31.4 %\n",
      "progress 31.5 %\n",
      "progress 31.6 %\n",
      "progress 31.7 %\n",
      "progress 31.8 %\n",
      "progress 31.900000000000002 %\n",
      "progress 32.0 %\n",
      "progress 32.1 %\n",
      "progress 32.2 %\n",
      "progress 32.300000000000004 %\n",
      "progress 32.4 %\n",
      "progress 32.5 %\n",
      "progress 32.6 %\n",
      "progress 32.7 %\n",
      "progress 32.800000000000004 %\n",
      "progress 32.9 %\n",
      "progress 33.0 %\n",
      "progress 33.1 %\n",
      "progress 33.2 %\n",
      "progress 33.300000000000004 %\n",
      "progress 33.4 %\n",
      "progress 33.5 %\n",
      "progress 33.6 %\n",
      "progress 33.7 %\n",
      "progress 33.800000000000004 %\n",
      "progress 33.900000000000006 %\n",
      "progress 34.0 %\n",
      "progress 34.1 %\n",
      "progress 34.2 %\n",
      "progress 34.300000000000004 %\n",
      "progress 34.4 %\n",
      "progress 34.5 %\n",
      "progress 34.599999999999994 %\n",
      "progress 34.699999999999996 %\n",
      "progress 34.8 %\n",
      "progress 34.9 %\n",
      "progress 35.0 %\n",
      "progress 35.099999999999994 %\n",
      "progress 35.199999999999996 %\n",
      "progress 35.3 %\n",
      "progress 35.4 %\n",
      "progress 35.5 %\n",
      "progress 35.6 %\n",
      "progress 35.699999999999996 %\n",
      "progress 35.8 %\n",
      "progress 35.9 %\n",
      "progress 36.0 %\n",
      "progress 36.1 %\n",
      "progress 36.199999999999996 %\n",
      "progress 36.3 %\n",
      "progress 36.4 %\n",
      "progress 36.5 %\n",
      "progress 36.6 %\n",
      "progress 36.7 %\n",
      "progress 36.8 %\n",
      "progress 36.9 %\n",
      "progress 37.0 %\n",
      "progress 37.1 %\n",
      "progress 37.2 %\n",
      "progress 37.3 %\n",
      "progress 37.4 %\n",
      "progress 37.5 %\n",
      "progress 37.6 %\n",
      "progress 37.7 %\n",
      "progress 37.8 %\n",
      "progress 37.9 %\n",
      "progress 38.0 %\n",
      "progress 38.1 %\n",
      "progress 38.2 %\n",
      "progress 38.3 %\n",
      "progress 38.4 %\n",
      "progress 38.5 %\n",
      "progress 38.6 %\n",
      "progress 38.7 %\n",
      "progress 38.800000000000004 %\n",
      "progress 38.9 %\n",
      "progress 39.0 %\n",
      "progress 39.1 %\n",
      "progress 39.2 %\n",
      "progress 39.300000000000004 %\n",
      "progress 39.4 %\n",
      "progress 39.5 %\n",
      "progress 39.6 %\n",
      "progress 39.7 %\n",
      "progress 39.800000000000004 %\n",
      "progress 39.900000000000006 %\n",
      "progress 40.0 %\n",
      "progress 40.1 %\n",
      "progress 40.2 %\n",
      "progress 40.300000000000004 %\n",
      "progress 40.400000000000006 %\n",
      "progress 40.5 %\n",
      "progress 40.6 %\n",
      "progress 40.699999999999996 %\n",
      "progress 40.8 %\n",
      "progress 40.9 %\n",
      "progress 41.0 %\n",
      "progress 41.099999999999994 %\n",
      "progress 41.199999999999996 %\n",
      "progress 41.3 %\n",
      "progress 41.4 %\n",
      "progress 41.5 %\n",
      "progress 41.6 %\n",
      "progress 41.699999999999996 %\n",
      "progress 41.8 %\n",
      "progress 41.9 %\n",
      "progress 42.0 %\n",
      "progress 42.1 %\n",
      "progress 42.199999999999996 %\n",
      "progress 42.3 %\n",
      "progress 42.4 %\n",
      "progress 42.5 %\n",
      "progress 42.6 %\n",
      "progress 42.699999999999996 %\n",
      "progress 42.8 %\n",
      "progress 42.9 %\n",
      "progress 43.0 %\n",
      "progress 43.1 %\n",
      "progress 43.2 %\n",
      "progress 43.3 %\n",
      "progress 43.4 %\n",
      "progress 43.5 %\n",
      "progress 43.6 %\n",
      "progress 43.7 %\n",
      "progress 43.8 %\n",
      "progress 43.9 %\n",
      "progress 44.0 %\n",
      "progress 44.1 %\n",
      "progress 44.2 %\n",
      "progress 44.3 %\n",
      "progress 44.4 %\n",
      "progress 44.5 %\n",
      "progress 44.6 %\n",
      "progress 44.7 %\n",
      "progress 44.800000000000004 %\n",
      "progress 44.9 %\n",
      "progress 45.0 %\n",
      "progress 45.1 %\n",
      "progress 45.2 %\n",
      "progress 45.300000000000004 %\n",
      "progress 45.4 %\n",
      "progress 45.5 %\n",
      "progress 45.6 %\n",
      "progress 45.7 %\n",
      "progress 45.800000000000004 %\n",
      "progress 45.9 %\n",
      "progress 46.0 %\n",
      "progress 46.1 %\n",
      "progress 46.2 %\n",
      "progress 46.300000000000004 %\n",
      "progress 46.400000000000006 %\n",
      "progress 46.5 %\n",
      "progress 46.6 %\n",
      "progress 46.7 %\n",
      "progress 46.800000000000004 %\n",
      "progress 46.9 %\n",
      "progress 47.0 %\n",
      "progress 47.099999999999994 %\n",
      "progress 47.199999999999996 %\n",
      "progress 47.3 %\n",
      "progress 47.4 %\n",
      "progress 47.5 %\n",
      "progress 47.599999999999994 %\n",
      "progress 47.699999999999996 %\n",
      "progress 47.8 %\n",
      "progress 47.9 %\n",
      "progress 48.0 %\n",
      "progress 48.1 %\n",
      "progress 48.199999999999996 %\n",
      "progress 48.3 %\n",
      "progress 48.4 %\n",
      "progress 48.5 %\n",
      "progress 48.6 %\n",
      "progress 48.699999999999996 %\n",
      "progress 48.8 %\n",
      "progress 48.9 %\n",
      "progress 49.0 %\n",
      "progress 49.1 %\n",
      "progress 49.2 %\n",
      "progress 49.3 %\n",
      "progress 49.4 %\n",
      "progress 49.5 %\n",
      "progress 49.6 %\n",
      "progress 49.7 %\n",
      "progress 49.8 %\n",
      "progress 49.9 %\n",
      "progress 50.0 %\n",
      "progress 50.1 %\n",
      "progress 50.2 %\n",
      "progress 50.3 %\n",
      "progress 50.4 %\n",
      "progress 50.5 %\n",
      "progress 50.6 %\n",
      "progress 50.7 %\n",
      "progress 50.8 %\n",
      "progress 50.9 %\n",
      "progress 51.0 %\n",
      "progress 51.1 %\n",
      "progress 51.2 %\n",
      "progress 51.300000000000004 %\n",
      "progress 51.4 %\n",
      "progress 51.5 %\n",
      "progress 51.6 %\n",
      "progress 51.7 %\n",
      "progress 51.800000000000004 %\n",
      "progress 51.9 %\n",
      "progress 52.0 %\n",
      "progress 52.1 %\n",
      "progress 52.2 %\n",
      "progress 52.300000000000004 %\n",
      "progress 52.400000000000006 %\n",
      "progress 52.5 %\n",
      "progress 52.6 %\n",
      "progress 52.7 %\n",
      "progress 52.800000000000004 %\n",
      "progress 52.900000000000006 %\n",
      "progress 53.0 %\n",
      "progress 53.1 %\n",
      "progress 53.2 %\n",
      "progress 53.300000000000004 %\n",
      "progress 53.400000000000006 %\n",
      "progress 53.5 %\n",
      "progress 53.6 %\n",
      "progress 53.7 %\n",
      "progress 53.800000000000004 %\n",
      "progress 53.900000000000006 %\n",
      "progress 54.0 %\n",
      "progress 54.1 %\n",
      "progress 54.2 %\n",
      "progress 54.300000000000004 %\n",
      "progress 54.400000000000006 %\n",
      "progress 54.50000000000001 %\n",
      "progress 54.6 %\n",
      "progress 54.7 %\n",
      "progress 54.800000000000004 %\n",
      "progress 54.900000000000006 %\n",
      "progress 55.00000000000001 %\n",
      "progress 55.1 %\n",
      "progress 55.2 %\n",
      "progress 55.300000000000004 %\n",
      "progress 55.400000000000006 %\n",
      "progress 55.50000000000001 %\n",
      "progress 55.60000000000001 %\n",
      "progress 55.7 %\n",
      "progress 55.800000000000004 %\n",
      "progress 55.900000000000006 %\n",
      "progress 56.00000000000001 %\n",
      "progress 56.10000000000001 %\n",
      "progress 56.2 %\n",
      "progress 56.3 %\n",
      "progress 56.39999999999999 %\n",
      "progress 56.49999999999999 %\n",
      "progress 56.599999999999994 %\n",
      "progress 56.699999999999996 %\n",
      "progress 56.8 %\n",
      "progress 56.89999999999999 %\n",
      "progress 56.99999999999999 %\n",
      "progress 57.099999999999994 %\n",
      "progress 57.199999999999996 %\n",
      "progress 57.3 %\n",
      "progress 57.4 %\n",
      "progress 57.49999999999999 %\n",
      "progress 57.599999999999994 %\n",
      "progress 57.699999999999996 %\n",
      "progress 57.8 %\n",
      "progress 57.9 %\n",
      "progress 57.99999999999999 %\n",
      "progress 58.099999999999994 %\n",
      "progress 58.199999999999996 %\n",
      "progress 58.3 %\n",
      "progress 58.4 %\n",
      "progress 58.5 %\n",
      "progress 58.599999999999994 %\n",
      "progress 58.699999999999996 %\n",
      "progress 58.8 %\n",
      "progress 58.9 %\n",
      "progress 59.0 %\n",
      "progress 59.099999999999994 %\n",
      "progress 59.199999999999996 %\n",
      "progress 59.3 %\n",
      "progress 59.4 %\n",
      "progress 59.5 %\n",
      "progress 59.599999999999994 %\n",
      "progress 59.699999999999996 %\n",
      "progress 59.8 %\n",
      "progress 59.9 %\n",
      "progress 60.0 %\n",
      "progress 60.099999999999994 %\n",
      "progress 60.199999999999996 %\n",
      "progress 60.3 %\n",
      "progress 60.4 %\n",
      "progress 60.5 %\n",
      "progress 60.6 %\n",
      "progress 60.699999999999996 %\n",
      "progress 60.8 %\n",
      "progress 60.9 %\n",
      "progress 61.0 %\n",
      "progress 61.1 %\n",
      "progress 61.199999999999996 %\n",
      "progress 61.3 %\n",
      "progress 61.4 %\n",
      "progress 61.5 %\n",
      "progress 61.6 %\n",
      "progress 61.7 %\n",
      "progress 61.8 %\n",
      "progress 61.9 %\n",
      "progress 62.0 %\n",
      "progress 62.1 %\n",
      "progress 62.2 %\n",
      "progress 62.3 %\n",
      "progress 62.4 %\n",
      "progress 62.5 %\n",
      "progress 62.6 %\n",
      "progress 62.7 %\n",
      "progress 62.8 %\n",
      "progress 62.9 %\n",
      "progress 63.0 %\n",
      "progress 63.1 %\n",
      "progress 63.2 %\n",
      "progress 63.3 %\n",
      "progress 63.4 %\n",
      "progress 63.5 %\n",
      "progress 63.6 %\n",
      "progress 63.7 %\n",
      "progress 63.800000000000004 %\n",
      "progress 63.9 %\n",
      "progress 64.0 %\n",
      "progress 64.1 %\n",
      "progress 64.2 %\n",
      "progress 64.3 %\n",
      "progress 64.4 %\n",
      "progress 64.5 %\n",
      "progress 64.60000000000001 %\n",
      "progress 64.7 %\n",
      "progress 64.8 %\n",
      "progress 64.9 %\n",
      "progress 65.0 %\n",
      "progress 65.10000000000001 %\n",
      "progress 65.2 %\n",
      "progress 65.3 %\n",
      "progress 65.4 %\n",
      "progress 65.5 %\n",
      "progress 65.60000000000001 %\n",
      "progress 65.7 %\n",
      "progress 65.8 %\n",
      "progress 65.9 %\n",
      "progress 66.0 %\n",
      "progress 66.10000000000001 %\n",
      "progress 66.2 %\n",
      "progress 66.3 %\n",
      "progress 66.4 %\n",
      "progress 66.5 %\n",
      "progress 66.60000000000001 %\n",
      "progress 66.7 %\n",
      "progress 66.8 %\n",
      "progress 66.9 %\n",
      "progress 67.0 %\n",
      "progress 67.10000000000001 %\n",
      "progress 67.2 %\n",
      "progress 67.30000000000001 %\n",
      "progress 67.4 %\n",
      "progress 67.5 %\n",
      "progress 67.60000000000001 %\n",
      "progress 67.7 %\n",
      "progress 67.80000000000001 %\n",
      "progress 67.9 %\n",
      "progress 68.0 %\n",
      "progress 68.10000000000001 %\n",
      "progress 68.2 %\n",
      "progress 68.30000000000001 %\n",
      "progress 68.4 %\n",
      "progress 68.5 %\n",
      "progress 68.60000000000001 %\n",
      "progress 68.7 %\n",
      "progress 68.8 %\n",
      "progress 68.89999999999999 %\n",
      "progress 69.0 %\n",
      "progress 69.1 %\n",
      "progress 69.19999999999999 %\n",
      "progress 69.3 %\n",
      "progress 69.39999999999999 %\n",
      "progress 69.5 %\n",
      "progress 69.6 %\n",
      "progress 69.69999999999999 %\n",
      "progress 69.8 %\n",
      "progress 69.89999999999999 %\n",
      "progress 70.0 %\n",
      "progress 70.1 %\n",
      "progress 70.19999999999999 %\n",
      "progress 70.3 %\n",
      "progress 70.39999999999999 %\n",
      "progress 70.5 %\n",
      "progress 70.6 %\n",
      "progress 70.7 %\n",
      "progress 70.8 %\n",
      "progress 70.89999999999999 %\n",
      "progress 71.0 %\n",
      "progress 71.1 %\n",
      "progress 71.2 %\n",
      "progress 71.3 %\n",
      "progress 71.39999999999999 %\n",
      "progress 71.5 %\n",
      "progress 71.6 %\n",
      "progress 71.7 %\n",
      "progress 71.8 %\n",
      "progress 71.89999999999999 %\n",
      "progress 72.0 %\n",
      "progress 72.1 %\n",
      "progress 72.2 %\n",
      "progress 72.3 %\n",
      "progress 72.39999999999999 %\n",
      "progress 72.5 %\n",
      "progress 72.6 %\n",
      "progress 72.7 %\n",
      "progress 72.8 %\n",
      "progress 72.89999999999999 %\n",
      "progress 73.0 %\n",
      "progress 73.1 %\n",
      "progress 73.2 %\n",
      "progress 73.3 %\n",
      "progress 73.4 %\n",
      "progress 73.5 %\n",
      "progress 73.6 %\n",
      "progress 73.7 %\n",
      "progress 73.8 %\n",
      "progress 73.9 %\n",
      "progress 74.0 %\n",
      "progress 74.1 %\n",
      "progress 74.2 %\n",
      "progress 74.3 %\n",
      "progress 74.4 %\n",
      "progress 74.5 %\n",
      "progress 74.6 %\n",
      "progress 74.7 %\n",
      "progress 74.8 %\n",
      "progress 74.9 %\n",
      "progress 75.0 %\n",
      "progress 75.1 %\n",
      "progress 75.2 %\n",
      "progress 75.3 %\n",
      "progress 75.4 %\n",
      "progress 75.5 %\n",
      "progress 75.6 %\n",
      "progress 75.7 %\n",
      "progress 75.8 %\n",
      "progress 75.9 %\n",
      "progress 76.0 %\n",
      "progress 76.1 %\n",
      "progress 76.2 %\n",
      "progress 76.3 %\n",
      "progress 76.4 %\n",
      "progress 76.5 %\n",
      "progress 76.6 %\n",
      "progress 76.7 %\n",
      "progress 76.8 %\n",
      "progress 76.9 %\n",
      "progress 77.0 %\n",
      "progress 77.10000000000001 %\n",
      "progress 77.2 %\n",
      "progress 77.3 %\n",
      "progress 77.4 %\n",
      "progress 77.5 %\n",
      "progress 77.60000000000001 %\n",
      "progress 77.7 %\n",
      "progress 77.8 %\n",
      "progress 77.9 %\n",
      "progress 78.0 %\n",
      "progress 78.10000000000001 %\n",
      "progress 78.2 %\n",
      "progress 78.3 %\n",
      "progress 78.4 %\n",
      "progress 78.5 %\n",
      "progress 78.60000000000001 %\n",
      "progress 78.7 %\n",
      "progress 78.8 %\n",
      "progress 78.9 %\n",
      "progress 79.0 %\n",
      "progress 79.10000000000001 %\n",
      "progress 79.2 %\n",
      "progress 79.3 %\n",
      "progress 79.4 %\n",
      "progress 79.5 %\n",
      "progress 79.60000000000001 %\n",
      "progress 79.7 %\n",
      "progress 79.80000000000001 %\n",
      "progress 79.9 %\n",
      "progress 80.0 %\n",
      "progress 80.10000000000001 %\n",
      "progress 80.2 %\n",
      "progress 80.30000000000001 %\n",
      "progress 80.4 %\n",
      "progress 80.5 %\n",
      "progress 80.60000000000001 %\n",
      "progress 80.7 %\n",
      "progress 80.80000000000001 %\n",
      "progress 80.9 %\n",
      "progress 81.0 %\n",
      "progress 81.10000000000001 %\n",
      "progress 81.2 %\n",
      "progress 81.3 %\n",
      "progress 81.39999999999999 %\n",
      "progress 81.5 %\n",
      "progress 81.6 %\n",
      "progress 81.69999999999999 %\n",
      "progress 81.8 %\n",
      "progress 81.89999999999999 %\n",
      "progress 82.0 %\n",
      "progress 82.1 %\n",
      "progress 82.19999999999999 %\n",
      "progress 82.3 %\n",
      "progress 82.39999999999999 %\n",
      "progress 82.5 %\n",
      "progress 82.6 %\n",
      "progress 82.69999999999999 %\n",
      "progress 82.8 %\n",
      "progress 82.89999999999999 %\n",
      "progress 83.0 %\n",
      "progress 83.1 %\n",
      "progress 83.2 %\n",
      "progress 83.3 %\n",
      "progress 83.39999999999999 %\n",
      "progress 83.5 %\n",
      "progress 83.6 %\n",
      "progress 83.7 %\n",
      "progress 83.8 %\n",
      "progress 83.89999999999999 %\n",
      "progress 84.0 %\n",
      "progress 84.1 %\n",
      "progress 84.2 %\n",
      "progress 84.3 %\n",
      "progress 84.39999999999999 %\n",
      "progress 84.5 %\n",
      "progress 84.6 %\n",
      "progress 84.7 %\n",
      "progress 84.8 %\n",
      "progress 84.89999999999999 %\n",
      "progress 85.0 %\n",
      "progress 85.1 %\n",
      "progress 85.2 %\n",
      "progress 85.3 %\n",
      "progress 85.39999999999999 %\n",
      "progress 85.5 %\n",
      "progress 85.6 %\n",
      "progress 85.7 %\n",
      "progress 85.8 %\n",
      "progress 85.9 %\n",
      "progress 86.0 %\n",
      "progress 86.1 %\n",
      "progress 86.2 %\n",
      "progress 86.3 %\n",
      "progress 86.4 %\n",
      "progress 86.5 %\n",
      "progress 86.6 %\n",
      "progress 86.7 %\n",
      "progress 86.8 %\n",
      "progress 86.9 %\n",
      "progress 87.0 %\n",
      "progress 87.1 %\n",
      "progress 87.2 %\n",
      "progress 87.3 %\n",
      "progress 87.4 %\n",
      "progress 87.5 %\n",
      "progress 87.6 %\n",
      "progress 87.7 %\n",
      "progress 87.8 %\n",
      "progress 87.9 %\n",
      "progress 88.0 %\n",
      "progress 88.1 %\n",
      "progress 88.2 %\n",
      "progress 88.3 %\n",
      "progress 88.4 %\n",
      "progress 88.5 %\n",
      "progress 88.6 %\n",
      "progress 88.7 %\n",
      "progress 88.8 %\n",
      "progress 88.9 %\n",
      "progress 89.0 %\n",
      "progress 89.1 %\n",
      "progress 89.2 %\n",
      "progress 89.3 %\n",
      "progress 89.4 %\n",
      "progress 89.5 %\n",
      "progress 89.60000000000001 %\n",
      "progress 89.7 %\n",
      "progress 89.8 %\n",
      "progress 89.9 %\n",
      "progress 90.0 %\n",
      "progress 90.10000000000001 %\n",
      "progress 90.2 %\n",
      "progress 90.3 %\n",
      "progress 90.4 %\n",
      "progress 90.5 %\n",
      "progress 90.60000000000001 %\n",
      "progress 90.7 %\n",
      "progress 90.8 %\n",
      "progress 90.9 %\n",
      "progress 91.0 %\n",
      "progress 91.10000000000001 %\n",
      "progress 91.2 %\n",
      "progress 91.3 %\n",
      "progress 91.4 %\n",
      "progress 91.5 %\n",
      "progress 91.60000000000001 %\n",
      "progress 91.7 %\n",
      "progress 91.8 %\n",
      "progress 91.9 %\n",
      "progress 92.0 %\n",
      "progress 92.10000000000001 %\n",
      "progress 92.2 %\n",
      "progress 92.30000000000001 %\n",
      "progress 92.4 %\n",
      "progress 92.5 %\n",
      "progress 92.60000000000001 %\n",
      "progress 92.7 %\n",
      "progress 92.80000000000001 %\n",
      "progress 92.9 %\n",
      "progress 93.0 %\n",
      "progress 93.10000000000001 %\n",
      "progress 93.2 %\n",
      "progress 93.30000000000001 %\n",
      "progress 93.4 %\n",
      "progress 93.5 %\n",
      "progress 93.60000000000001 %\n",
      "progress 93.7 %\n",
      "progress 93.8 %\n",
      "progress 93.89999999999999 %\n",
      "progress 94.0 %\n",
      "progress 94.1 %\n",
      "progress 94.19999999999999 %\n",
      "progress 94.3 %\n",
      "progress 94.39999999999999 %\n",
      "progress 94.5 %\n",
      "progress 94.6 %\n",
      "progress 94.69999999999999 %\n",
      "progress 94.8 %\n",
      "progress 94.89999999999999 %\n",
      "progress 95.0 %\n",
      "progress 95.1 %\n",
      "progress 95.19999999999999 %\n",
      "progress 95.3 %\n",
      "progress 95.39999999999999 %\n",
      "progress 95.5 %\n",
      "progress 95.6 %\n",
      "progress 95.7 %\n",
      "progress 95.8 %\n",
      "progress 95.89999999999999 %\n",
      "progress 96.0 %\n",
      "progress 96.1 %\n",
      "progress 96.2 %\n",
      "progress 96.3 %\n",
      "progress 96.39999999999999 %\n",
      "progress 96.5 %\n",
      "progress 96.6 %\n",
      "progress 96.7 %\n",
      "progress 96.8 %\n",
      "progress 96.89999999999999 %\n",
      "progress 97.0 %\n",
      "progress 97.1 %\n",
      "progress 97.2 %\n",
      "progress 97.3 %\n",
      "progress 97.39999999999999 %\n",
      "progress 97.5 %\n",
      "progress 97.6 %\n",
      "progress 97.7 %\n",
      "progress 97.8 %\n",
      "progress 97.89999999999999 %\n",
      "progress 98.0 %\n",
      "progress 98.1 %\n",
      "progress 98.2 %\n",
      "progress 98.3 %\n",
      "progress 98.4 %\n",
      "progress 98.5 %\n",
      "progress 98.6 %\n",
      "progress 98.7 %\n",
      "progress 98.8 %\n",
      "progress 98.9 %\n",
      "progress 99.0 %\n",
      "progress 99.1 %\n",
      "progress 99.2 %\n",
      "progress 99.3 %\n",
      "progress 99.4 %\n",
      "progress 99.5 %\n",
      "progress 99.6 %\n",
      "progress 99.7 %\n",
      "progress 99.8 %\n",
      "progress 99.9 %\n",
      "progress 100.0 %\n"
     ]
    }
   ],
   "source": [
    "# explainer = lime.lime_tabular.LimeTabularExplainer(test, feature_names= list(test2.columns.values) , class_names=label , discretize_continuous=True)\n",
    "\n",
    "#creating dict \n",
    "feat_list = features\n",
    "print(feat_list)\n",
    "\n",
    "# feat_dict = dict.fromkeys(feat_list, 0)\n",
    "c = 0\n",
    "\n",
    "num_columns = len(feat_list)\n",
    "feature_name = feat_list\n",
    "feature_name_2 = feat_list\n",
    "feature_name_2.sort()\n",
    "feature_name.sort()\n",
    "feature_val = []\n",
    "feature_val_2 = []\n",
    "feature_val_abs = []\n",
    "feature_val_abs_2 = []\n",
    "\n",
    "# position = y_labels.index(rf.predict(Dos_sample2))\n",
    "# position =  np.argmax(rf.predict_proba(((Dos_sample2))))\n",
    "# print(len(y_labels))\n",
    "# print(rf.predict(Dos_sample2))\n",
    "\n",
    "\n",
    "# sample = Dos_sample\n",
    "# samples = 1 \n",
    "# sample = PS_sample\n",
    "\n",
    "\n",
    "for i in range(0,num_columns): \n",
    "    feature_val.append(0)\n",
    "    feature_val_abs.append(0)\n",
    "    feature_val_2.append(0)\n",
    "    feature_val_abs_2.append(0)    \n",
    "\n",
    "# # i = sample\n",
    "#     # exp = explainer.explain_instance(test[i], rf.predict_proba)\n",
    "\n",
    "#     exp = explainer.explain_instance(sample, rf.predict_proba, num_features=num_columns, top_labels=len(y_labels))\n",
    "#     exp.show_in_notebook(show_table=True, show_all=True)\n",
    "# Let's just look at a the first example in the test set\n",
    "# ex_indc = np.random.choice(xtest.shape[0])\n",
    "\n",
    "samples = 1000\n",
    "\n",
    "for j in range(0,samples):\n",
    "    ex_indc = j\n",
    "    # print(xtest.shape[0])\n",
    "    # indices = np.where(ytest == 1)\n",
    "    # index_positions = indices[0]\n",
    "    # print(index_positions)\n",
    "    # ex_indc = index_positions[0]\n",
    "    # To get a baseline, we'll look at LIME applied to the biased model f\n",
    "    normal_explainer = lime.lime_tabular.LimeTabularExplainer(xtrain,feature_names=adv_lime.get_column_names(),\n",
    "                                                        discretize_continuous=False)\n",
    "\n",
    "    normal_exp = normal_explainer.explain_instance(xtest[ex_indc], racist_model_f().predict_proba).as_list()\n",
    "\n",
    "    # print('------------------------------------------------------------------')\n",
    "    # print('real value', ytest[ex_indc:ex_indc+1])\n",
    "    # print(ytest)\n",
    "    # print('------------------------------------------------------------------')\n",
    "    # print('------------------------------------------------------------------')\n",
    "    # print(xtest[ex_indc])\n",
    "    # print(xtest)\n",
    "    # print('predicted value', racist_model_f().predict(xtest)[ex_indc])\n",
    "\n",
    "    # print('------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    # pred = racist_model_f().predict_proba(xtest[ex_indc])                             \n",
    "    # print('prediction', pred)\n",
    "    # print (\"Explanation on biased f:\\n\",normal_exp[:3],\"\\n\\n\")\n",
    "\n",
    "    # Now, lets look at the explanations on the adversarial model \n",
    "    adv_explainer = lime.lime_tabular.LimeTabularExplainer(xtrain,feature_names=adv_lime.get_column_names(), \n",
    "                                                    discretize_continuous=False)\n",
    "\n",
    "    adv_exp = adv_explainer.explain_instance(xtest[ex_indc], adv_lime.predict_proba).as_list()\n",
    "\n",
    "    # print (\"Explanation on adversarial model:\\n\",adv_exp[:3],\"\\n\")\n",
    "\n",
    "    # print(\"Prediction fidelity: {0:3.2}\".format(adv_lime.fidelity(xtest[ex_indc:ex_indc+1])))\n",
    "\n",
    "    lime_list = adv_exp\n",
    "    lime_list_2 = normal_exp\n",
    "    lime_list.sort()\n",
    "    lime_list_2.sort()\n",
    "    # print(lime_list)\n",
    "\n",
    "    for i in range(0,len(lime_list)):\n",
    "    #---------------------------------------------------\n",
    "    #fix\n",
    "        my_string = lime_list[i][0]\n",
    "        for index, char in enumerate(my_string):\n",
    "            if char.isalpha():\n",
    "                first_letter_index = index\n",
    "                break  # Exit the loop when the first letter is found\n",
    "        my_string = my_string[first_letter_index:]\n",
    "        modified_tuple = list(lime_list[i])\n",
    "        modified_tuple[0] = my_string\n",
    "        lime_list[i] = tuple(modified_tuple)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(0,len(lime_list_2)):\n",
    "    #---------------------------------------------------\n",
    "    #fix\n",
    "        my_string_2 = lime_list_2[i][0]\n",
    "        for index, char in enumerate(my_string_2):\n",
    "            if char.isalpha():\n",
    "                first_letter_index_2 = index\n",
    "                break  # Exit the loop when the first letter is found  \n",
    "        my_string_2 = my_string_2[first_letter_index_2:]\n",
    "        modified_tuple_2 = list(lime_list_2[i])\n",
    "        modified_tuple_2[0] = my_string_2\n",
    "        lime_list_2[i] = tuple(modified_tuple_2)\n",
    "            \n",
    "    #---------------------------------------------------\n",
    "\n",
    "    lime_list.sort()\n",
    "    lime_list_2.sort()\n",
    "    #print(lime_list)\n",
    "    # for j in range (0,num_columns): feature_val[j]+= abs(lime_list[j][1])\n",
    "    for j in range (0,len(lime_list)):feature_val_abs[j] = abs(lime_list[j][1])\n",
    "    for j in range (0,len(lime_list)):feature_val_abs_2[j] = abs(lime_list_2[j][1])\n",
    "    for j in range (0,len(lime_list)):feature_val[j] = lime_list[j][1]\n",
    "    for j in range (0,len(lime_list)):feature_val_2[j] = lime_list_2[j][1]\n",
    "    c = c + 1 \n",
    "    print ('progress',100*(c/samples),'%')\n",
    "\n",
    "\n",
    "\n",
    "    # Define the number you want to divide by\n",
    "    divider = samples\n",
    "\n",
    "    # Use a list comprehension to divide all elements by the same number\n",
    "    feature_val = [x / divider for x in feature_val]\n",
    "    feature_val_2 = [x / divider for x in feature_val_2]\n",
    "\n",
    "    # for item1, item2 in zip(feature_name, feature_val):\n",
    "    #     print(item1, item2)\n",
    "\n",
    "\n",
    "    # Use zip to combine the two lists, sort based on list1, and then unzip them\n",
    "    zipped_lists = list(zip(feature_name, feature_val,feature_val_abs))\n",
    "    zipped_lists.sort(key=lambda x: x[2],reverse=True)\n",
    "\n",
    "    zipped_lists_2 = list(zip(feature_name_2, feature_val_2,feature_val_abs_2))\n",
    "    zipped_lists_2.sort(key=lambda x: x[2],reverse=True)\n",
    "\n",
    "    # Convert the sorted result back into separate lists\n",
    "    sorted_list1, sorted_list2,sorted_list3 = [list(x) for x in zip(*zipped_lists)]\n",
    "\n",
    "    sorted_list1_2, sorted_list2_2,sorted_list3_2 = [list(x) for x in zip(*zipped_lists_2)]\n",
    "\n",
    "    # print('Adversarial')\n",
    "    # print(sorted_list1)\n",
    "    # print(sorted_list2)\n",
    "    # print(sorted_list3)\n",
    "\n",
    "    # print('Biased')\n",
    "    # print('----------------------------------------------------------------------------------------------------------------')\n",
    "    # print(sorted_list1_2)\n",
    "    # print(sorted_list2_2)\n",
    "    # print(sorted_list3_2)\n",
    "    # print('----------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "    biased_df = pd.DataFrame({\n",
    "            'shap_values': sorted_list2_2,\n",
    "            'shap_values_abs': sorted_list3_2,\n",
    "            'features': sorted_list1_2  \n",
    "        })\n",
    "    attack1_df = pd.DataFrame({\n",
    "            'shap_values': sorted_list2 ,\n",
    "            'shap_values_abs':sorted_list3,\n",
    "            'features':  sorted_list1  \n",
    "        })\n",
    "    # info = [adv_shap_values,features]\n",
    "\n",
    "    attack1_df.sort_values(by=['shap_values_abs'], ascending=False,inplace=True)\n",
    "    biased_df.sort_values(by=['shap_values_abs'], ascending=False,inplace=True)\n",
    "\n",
    "    attack1_df = attack1_df.reset_index(drop=True)\n",
    "    biased_df = biased_df.reset_index(drop=True)\n",
    "\n",
    "    # print('Attack')\n",
    "    # print('------------------------------')\n",
    "    # print (attack1_df)\n",
    "    # print('Biased')\n",
    "    # print('------------------------------')\n",
    "    # print(biased_df)\n",
    "\n",
    "\n",
    "    #biased columns\n",
    "\n",
    "    #For 1st position\n",
    "    # print(biased_df['shap_values'][0] )\n",
    "    if biased_df['features'][0] == biased_feature: dict_biased_1['Biased'] = dict_biased_1['Biased'] + 1\n",
    "    elif biased_df['features'][0] == Unrelated_1_feature: dict_biased_1['Unrelated_1'] = dict_biased_1['Unrelated_1'] + 1\n",
    "    # elif biased_df['features'][0] == Unrelated_2_feature: dict_biased_1['Unrelated_2'] = dict_biased_1['Unrelated_2'] + 1\n",
    "    else: dict_biased_1['Others'] = dict_biased_1['Others'] + 1\n",
    "\n",
    "    #For 2st position\n",
    "    if biased_df['features'][1] == biased_feature: dict_biased_2['Biased'] = dict_biased_2['Biased'] + 1\n",
    "    elif biased_df['features'][1] == Unrelated_1_feature: dict_biased_2['Unrelated_1'] = dict_biased_2['Unrelated_1'] + 1\n",
    "    # elif biased_df['features'][1] == Unrelated_2_feature: dict_biased_2['Unrelated_2'] = dict_biased_2['Unrelated_2'] + 1\n",
    "    else: dict_biased_2['Others'] = dict_biased_2['Others'] + 1\n",
    "\n",
    "    #For 3st position\n",
    "    if biased_df['features'][2] == biased_feature: dict_biased_3['Biased'] = dict_biased_3['Biased'] + 1\n",
    "    elif biased_df['features'][2] == Unrelated_1_feature: dict_biased_3['Unrelated_1'] = dict_biased_3['Unrelated_1'] + 1\n",
    "    # elif biased_df['features'][2] == Unrelated_2_feature: dict_biased_3['Unrelated_2'] = dict_biased_3['Unrelated_2'] + 1\n",
    "    else: dict_biased_3['Others'] = dict_biased_3['Others'] + 1\n",
    "\n",
    "\n",
    "    #Attack 1 columns\n",
    "\n",
    "    #For 1st position\n",
    "    if attack1_df['features'][0] == biased_feature: attack_1['Biased'] = attack_1['Biased'] + 1\n",
    "    elif attack1_df['features'][0] == Unrelated_1_feature: attack_1['Unrelated_1'] = attack_1['Unrelated_1'] + 1\n",
    "    # elif attack1_df['features'][0] == Unrelated_2_feature: attack_1['Unrelated_2'] = attack_1['Unrelated_2'] + 1\n",
    "    else: attack_1['Others'] = attack_1['Others'] + 1\n",
    "\n",
    "    #For 2st position\n",
    "    if attack1_df['features'][1] == biased_feature: attack_2['Biased'] = attack_2['Biased'] + 1\n",
    "    elif attack1_df['features'][1] == Unrelated_1_feature: attack_2['Unrelated_1'] = attack_2['Unrelated_1'] + 1\n",
    "    # elif attack1_df['features'][1] == Unrelated_2_feature: attack_2['Unrelated_2'] = attack_2['Unrelated_2'] + 1\n",
    "    else: attack_2['Others'] = attack_2['Others'] + 1\n",
    "\n",
    "    #For 3st position\n",
    "    if attack1_df['features'][2] == biased_feature: attack_3['Biased'] = attack_3['Biased'] + 1\n",
    "    elif attack1_df['features'][2] == Unrelated_1_feature: attack_3['Unrelated_1'] = attack_3['Unrelated_1'] + 1\n",
    "    # elif attack1_df['features'][2] == Unrelated_2_feature: attack_3['Unrelated_2'] = attack_3['Unrelated_2'] + 1\n",
    "    else: attack_3['Others'] = attack_3['Others'] + 1\n",
    "\n",
    "# for item1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "print(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31494"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb44f5c7ab524637bd9bc43cbfd5d05c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'FloatProgress' object has no attribute 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2554795/191697926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Explain the biased model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mbiased_kernel_explainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKernelExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mracist_model_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground_distribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mbiased_shap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbiased_kernel_explainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_examine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mto_examine\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# print(biased_kernel_explainer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_kernel.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mexplanations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"silent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# Print initial bar state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mcolour\u001b[0;34m(self, bar_color)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'container'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FloatProgress' object has no attribute 'style'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Biased': 1119, 'Unrelated_1': 0.032, 'Unrelated_2': 0.0, 'Others': 90}\n",
      "{'Biased': 0.141, 'Unrelated_1': 0.031, 'Unrelated_2': 0.0, 'Others': 1.068}\n",
      "{'Biased': 0.134, 'Unrelated_1': 0.034, 'Unrelated_2': 0.0, 'Others': 1.072}\n",
      "--------------\n",
      "{'Biased': 0.027, 'Unrelated_1': 1132, 'Unrelated_2': 0.0, 'Others': 81}\n",
      "{'Biased': 0.142, 'Unrelated_1': 0.035, 'Unrelated_2': 0.0, 'Others': 1.063}\n",
      "{'Biased': 0.165, 'Unrelated_1': 0.021, 'Unrelated_2': 0.0, 'Others': 1.054}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dict_biased_1 )\n",
    "print(dict_biased_2 )\n",
    "print(dict_biased_3 )\n",
    "print('--------------')\n",
    "\n",
    "print(attack_1 )\n",
    "print(attack_2 )\n",
    "print(attack_3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized\n",
    "keys = ['Biased','Unrelated_1','Unrelated_2','Others' ]\n",
    "for x in keys:\n",
    "\n",
    "    dict_biased_1[x] = dict_biased_1[x]/samples\n",
    "    dict_biased_2[x] = dict_biased_2[x]/samples\n",
    "    dict_biased_3[x] = dict_biased_3[x]/samples\n",
    "\n",
    "    attack_1[x] = attack_1[x]/samples\n",
    "    attack_2[x] = attack_2[x]/samples\n",
    "    attack_3[x] = attack_3[x]/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Biased': 0.319, 'Unrelated_1': 0.032, 'Unrelated_2': 0.0, 'Others': 0.89}\n",
      "{'Biased': 0.141, 'Unrelated_1': 0.031, 'Unrelated_2': 0.0, 'Others': 1.068}\n",
      "{'Biased': 0.134, 'Unrelated_1': 0.034, 'Unrelated_2': 0.0, 'Others': 1.072}\n",
      "--------------\n",
      "{'Biased': 0.027, 'Unrelated_1': 0.032, 'Unrelated_2': 0.0, 'Others': 1.181}\n",
      "{'Biased': 0.142, 'Unrelated_1': 0.035, 'Unrelated_2': 0.0, 'Others': 1.063}\n",
      "{'Biased': 0.165, 'Unrelated_1': 0.021, 'Unrelated_2': 0.0, 'Others': 1.054}\n"
     ]
    }
   ],
   "source": [
    "print(dict_biased_1 )\n",
    "print(dict_biased_2 )\n",
    "print(dict_biased_3 )\n",
    "print('--------------')\n",
    "print(attack_1 )\n",
    "print(attack_2 )\n",
    "print(attack_3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing package\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "total = samples \n",
    "# create data\n",
    "x = ['1st', '2nd', '3rd']\n",
    "y1 = np.array([dict_biased_1['Biased'], dict_biased_2['Biased'], dict_biased_3['Biased']])\n",
    "y2 = np.array([dict_biased_1['Unrelated_1'], dict_biased_2['Unrelated_1'], dict_biased_3['Unrelated_1']])\n",
    "y3 = np.array([dict_biased_1['Others'], dict_biased_2['Others'], dict_biased_3['Others']])\n",
    "y4 = np.array([dict_biased_1['Unrelated_2'], dict_biased_2['Unrelated_2'],dict_biased_3['Unrelated_2']])\n",
    "\n",
    "\n",
    "# plot bars in stack manner\n",
    "plt.bar(x, y1, color='r')\n",
    "plt.bar(x, y2, bottom=y1, color='b')\n",
    "plt.bar(x, y3, bottom=y1+y2, color='grey')\n",
    "# plt.bar(x, y4, bottom=y1+y2+y3, color='g')\n",
    "plt.xlabel(\"Features Importance Rank\", fontsize = 18)\n",
    "plt.ylabel(\"% Ocurrence\", fontsize = 18)\n",
    "# plt.legend([\"Biased\", \"Unrelated\", \"Unrelated_2\", \"Others\"])\n",
    "plt.legend([\"Biased\", \"Unrelated\", \"Others\"], fontsize = 14)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing package\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "total = samples \n",
    "# create data\n",
    "x = ['1st', '2nd', '3rd']\n",
    "y1 = np.array([attack_1['Biased'], attack_2['Biased'], attack_3['Biased']])\n",
    "y2 = np.array([attack_1['Unrelated_1'], attack_2['Unrelated_1'], attack_3['Unrelated_1']])\n",
    "y3 = np.array([attack_1['Others'], attack_2['Others'],attack_3['Others']])\n",
    "# y4 = np.array([attack_1['Unrelated_2'], attack_2['Unrelated_2'], attack_3['Unrelated_2']])\n",
    "\n",
    "# plot bars in stack manner\n",
    "plt.bar(x, y1, color='r')\n",
    "plt.bar(x, y2, bottom=y1, color='b')\n",
    "plt.bar(x, y3, bottom=y1+y2, color='grey')\n",
    "# plt.bar(x, y4, bottom=y1+y2+y3, color='g')\n",
    "plt.xlabel(\"Features Importance Rank\", fontsize = 18)\n",
    "plt.ylabel(\"% Ocurrence\", fontsize = 18)\n",
    "# plt.legend([\"Biased\", \"Unrelated_1\", \"Unrelated_2\", \"Others\"])\n",
    "plt.legend([\"Biased\", \"Unrelated_1\", \"Others\"], fontsize = 14)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
